---
title: "Practical Machine Learning: Week 4 Course Project"
author: "David Arcos"
date: "2023-04-20"
output: html_document  
---

# Introduction

This is the final project for Courseraâ€™s Practical Machine Learning course, as part of the Data Science Specialization offered by John Hopkins University.

Using devices such as Jawbone Up, Nike FuelBand, and Fitbitwe, in this project will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise using as predictor the ***classe*** variable. For this purpose, we will run **4** different models using k-folds cross validation and compare their overall accuracy by using a validation set randomly selected from the training data. Based on the accuracy, we use the best model to predict 20 cases using the test dataset. 

More information is available from the website here: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

# Data Sources

The training data for this project is available here: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data is available here:  
[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)  
 
The data for this project comes from this original source: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har). If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.  

# Loading Data and Libraries
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
library(lattice)
library(ggplot2)
library(caret)
library(kernlab)
library(rattle)
library(corrplot)
set.seed(1234)
```
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
traincsv <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
dim(traincsv)
```
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
testcsv <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
dim(testcsv)
```

# Data Cleaning 

For cleaning the training dataset, **first** we will remove the variables which consist mostly of NA (with this, we will remove about 100 variables which contain more than 90% of NAs as observations).

```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
traincsv <- traincsv[,colMeans(is.na(traincsv)) == 0] 
# Keep those variables that doesn't have any NA
```
**Second**, we will remove metadata variables, which happen to be the first **7** variables.
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
traincsv <- traincsv[,-c(1:7)]
```
**Third**, we will remove those variable that don't have much variability, for instance, zero or near-zero variance, by using the `nearZeroVar` command. As we can see, each of the 53 variables left in the training dataset contribute with some variability. 
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
nvz <- nearZeroVar(traincsv, saveMetrics = T)
```

# Data Partitioning

Now that we have removed unnecessary variables, we can proceed now by spliting the training dataset into a validation and sub training set with a 70/30 proportion. The testing set will be left alone, and used for the final testing.
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
inTrain <- createDataPartition(y=traincsv$classe, p=0.7, list=F)
train <- traincsv[inTrain,]
valid <- traincsv[-inTrain,]
dim(train); dim(valid)
```

Now, we can test 4 different types of models including: **Decision Trees**, **Random forest**, **Gradient Boosted Trees** and **Support Vector Machine**. For this, we set up the training controls so that we can use a 3-fold cross-validation.
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
control <- trainControl(method="cv", number=3, verboseIter=F)
```

# Testing models

## Decision trees
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
mod_trees <- train(classe~., data=train, method="rpart", 
                   trControl = control, tuneLength = 5)
pred_trees <- predict(mod_trees, valid)
cmtrees <- confusionMatrix(pred_trees, factor(valid$classe))
cmtrees
```
## Random forest
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
mod_rf <- train(classe~., data=train, method="rf", 
                trControl = control, tuneLength = 5)
pred_rf <- predict(mod_rf, valid)
cmrf <- confusionMatrix(pred_rf, factor(valid$classe))
cmrf
```
## Gradient boosted trees
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
mod_gbm <- train(classe~., data=train, method="gbm", 
                 trControl = control, tuneLength = 5, verbose = F)
pred_gbm <- predict(mod_gbm, valid)
cmgbm <- confusionMatrix(pred_gbm, factor(valid$classe))
cmgbm
```
## Support vector machine
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
mod_svm <- train(classe~., data=train, method="svmLinear", 
                 trControl = control, tuneLength = 5, verbose = F)

pred_svm <- predict(mod_svm, valid)
cmsvm <- confusionMatrix(pred_svm, factor(valid$classe))
cmsvm
```

# Results
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
models <- c("Tree", "RF", "GBM", "SVM")
# Accuracy
accuracy <- round(c(cmtrees$overall[1], cmrf$overall[1], cmgbm$overall[1], cmsvm$overall[1]),3) 
# Out-of-sample error
oos_error <- 1 - accuracy
data.frame(accuracy = accuracy, oos_error = oos_error, row.names = models)
```
The best model for predict this dataset is the **Random Forest** model with a 99.6% accuracy rate and a 0.4% out-of-sample error rate, followed by the **Gradient Boost Model** with a 99.1% accuracy rate and a 0.9% out-of-sample rate.

# Predictions on the Test dataset
Finally, we can use the **Random Forest** model to predict the 5 leves on the *"classe"* variable in the test set for 20 different observations.
```{r, echo=TRUE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
pred <- predict(mod_rf, testcsv)
print(pred)
```

# Appendix

### Appendix 1. Correlation plot between the 53 variables of the training dataset
```{r, echo=FALSE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
corrPlot <- cor(train[, -length(names(train))])
corrplot(corrPlot,
         order = "FPC", # Organized using the first principal component
         method = "shade",
         type = "lower", 
         tl.col = "black", tl.srt = 35, 
         outline = T,tl.cex = 0.6,
         addgrid.col = "black")
```

### Appendix 2. Decision tree plot
```{r, echo=FALSE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
fancyRpartPlot(mod_trees$finalModel)
```

### Appendix 3. Random Forest plot
```{r, echo=FALSE, eval=TRUE, include =TRUE, warning=FALSE, error=FALSE, message=FALSE}
plot(mod_rf)
```